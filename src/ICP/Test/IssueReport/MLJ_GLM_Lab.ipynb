{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using MLJ with the GLM package\n",
    "\n",
    "This juypter lab showcases MLJ in particular using the popular [GLM](https://github.com/JuliaStats/GLM.jl) Julia package. We are using the CollegeDistance dataset from the [AER](https://cran.r-project.org/web/packages/AER/index.html) package in R.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "┌ Info: A model type \"LinearRegressor\" is already loaded. \n",
      "│ No new code loaded. \n",
      "└ @ MLJModels C:\\Users\\BCP\\.juliapro\\JuliaPro_v1.4.1-1\\packages\\MLJModels\\gHake\\src\\loading.jl:43\n",
      "┌ Info: A model type \"LinearBinaryClassifier\" is already loaded. \n",
      "│ No new code loaded. \n",
      "└ @ MLJModels C:\\Users\\BCP\\.juliapro\\JuliaPro_v1.4.1-1\\packages\\MLJModels\\gHake\\src\\loading.jl:43\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LinearBinaryClassifier(\n",
       "    fit_intercept = true,\n",
       "    link = GLM.LogitLink())\u001b[34m @ 8…34\u001b[39m"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "using Queryverse, MLJ, CategoricalArrays, PrettyPrinting\n",
    "@load LinearRegressor pkg = GLM\n",
    "@load LinearBinaryClassifier pkg=GLM \n",
    "##########@load LinearCountRegressor pkg=GLM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reading the data\n",
    "\n",
    "The the CollegeDistance dataset was stored in a CSV file.  Here, we read the input file.   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table class=\"data-frame\"><thead><tr><th></th><th>Y</th></tr><tr><th></th><th>Int64</th></tr></thead><tbody><p>4,739 rows × 1 columns</p><tr><th>1</th><td>0</td></tr><tr><th>2</th><td>0</td></tr><tr><th>3</th><td>0</td></tr><tr><th>4</th><td>0</td></tr><tr><th>5</th><td>0</td></tr><tr><th>6</th><td>0</td></tr><tr><th>7</th><td>0</td></tr><tr><th>8</th><td>0</td></tr><tr><th>9</th><td>0</td></tr><tr><th>10</th><td>0</td></tr><tr><th>11</th><td>0</td></tr><tr><th>12</th><td>0</td></tr><tr><th>13</th><td>0</td></tr><tr><th>14</th><td>1</td></tr><tr><th>15</th><td>0</td></tr><tr><th>16</th><td>0</td></tr><tr><th>17</th><td>0</td></tr><tr><th>18</th><td>0</td></tr><tr><th>19</th><td>1</td></tr><tr><th>20</th><td>1</td></tr><tr><th>21</th><td>0</td></tr><tr><th>22</th><td>0</td></tr><tr><th>23</th><td>0</td></tr><tr><th>24</th><td>1</td></tr><tr><th>25</th><td>0</td></tr><tr><th>26</th><td>0</td></tr><tr><th>27</th><td>0</td></tr><tr><th>28</th><td>0</td></tr><tr><th>29</th><td>1</td></tr><tr><th>30</th><td>0</td></tr><tr><th>&vellip;</th><td>&vellip;</td></tr></tbody></table>"
      ],
      "text/latex": [
       "\\begin{tabular}{r|c}\n",
       "\t& Y\\\\\n",
       "\t\\hline\n",
       "\t& Int64\\\\\n",
       "\t\\hline\n",
       "\t1 & 0 \\\\\n",
       "\t2 & 0 \\\\\n",
       "\t3 & 0 \\\\\n",
       "\t4 & 0 \\\\\n",
       "\t5 & 0 \\\\\n",
       "\t6 & 0 \\\\\n",
       "\t7 & 0 \\\\\n",
       "\t8 & 0 \\\\\n",
       "\t9 & 0 \\\\\n",
       "\t10 & 0 \\\\\n",
       "\t11 & 0 \\\\\n",
       "\t12 & 0 \\\\\n",
       "\t13 & 0 \\\\\n",
       "\t14 & 1 \\\\\n",
       "\t15 & 0 \\\\\n",
       "\t16 & 0 \\\\\n",
       "\t17 & 0 \\\\\n",
       "\t18 & 0 \\\\\n",
       "\t19 & 1 \\\\\n",
       "\t20 & 1 \\\\\n",
       "\t21 & 0 \\\\\n",
       "\t22 & 0 \\\\\n",
       "\t23 & 0 \\\\\n",
       "\t24 & 1 \\\\\n",
       "\t25 & 0 \\\\\n",
       "\t26 & 0 \\\\\n",
       "\t27 & 0 \\\\\n",
       "\t28 & 0 \\\\\n",
       "\t29 & 1 \\\\\n",
       "\t30 & 0 \\\\\n",
       "\t$\\dots$ & $\\dots$ \\\\\n",
       "\\end{tabular}\n"
      ],
      "text/plain": [
       "4739×1 DataFrame\n",
       "│ Row  │ Y     │\n",
       "│      │ \u001b[90mInt64\u001b[39m │\n",
       "├──────┼───────┤\n",
       "│ 1    │ 0     │\n",
       "│ 2    │ 0     │\n",
       "│ 3    │ 0     │\n",
       "│ 4    │ 0     │\n",
       "│ 5    │ 0     │\n",
       "│ 6    │ 0     │\n",
       "│ 7    │ 0     │\n",
       "│ 8    │ 0     │\n",
       "│ 9    │ 0     │\n",
       "│ 10   │ 0     │\n",
       "⋮\n",
       "│ 4729 │ 1     │\n",
       "│ 4730 │ 1     │\n",
       "│ 4731 │ 0     │\n",
       "│ 4732 │ 1     │\n",
       "│ 4733 │ 0     │\n",
       "│ 4734 │ 0     │\n",
       "│ 4735 │ 0     │\n",
       "│ 4736 │ 0     │\n",
       "│ 4737 │ 0     │\n",
       "│ 4738 │ 1     │\n",
       "│ 4739 │ 0     │"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dfX = DataFrame(Queryverse.load(\"C:\\\\Users\\\\BCP\\\\github\\\\ICP\\\\Test\\\\X3.csv\"))\n",
    "\n",
    "dfYbinary = DataFrame(Queryverse.load(\"C:\\\\Users\\\\BCP\\\\github\\\\ICP\\\\Test\\\\Y3.csv\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining the Linear Model\n",
    "\n",
    "Let see how many MLJ models handle our kind of target which is the y variable. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3-element Array{NamedTuple{(:name, :package_name, :is_supervised, :docstring, :hyperparameter_ranges, :hyperparameter_types, :hyperparameters, :implemented_methods, :is_pure_julia, :is_wrapper, :load_path, :package_license, :package_url, :package_uuid, :prediction_type, :supports_online, :supports_weights, :input_scitype, :target_scitype, :output_scitype),T} where T<:Tuple,1}:\n",
       " (name = EvoTreeCount, package_name = EvoTrees, ... )\n",
       " (name = LinearCountRegressor, package_name = GLM, ... )\n",
       " (name = XGBoostCount, package_name = XGBoost, ... )"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    " ms = models() do m AbstractVector{Count}<: m.target_scitype end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "52-element Array{NamedTuple{(:name, :package_name, :is_supervised, :docstring, :hyperparameter_ranges, :hyperparameter_types, :hyperparameters, :implemented_methods, :is_pure_julia, :is_wrapper, :load_path, :package_license, :package_url, :package_uuid, :prediction_type, :supports_online, :supports_weights, :input_scitype, :target_scitype, :output_scitype),T} where T<:Tuple,1}:\n",
       " (name = ARDRegressor, package_name = ScikitLearn, ... )\n",
       " (name = AdaBoostRegressor, package_name = ScikitLearn, ... )\n",
       " (name = BaggingRegressor, package_name = ScikitLearn, ... )\n",
       " (name = BayesianRidgeRegressor, package_name = ScikitLearn, ... )\n",
       " (name = ConstantRegressor, package_name = MLJModels, ... )\n",
       " (name = DecisionTreeRegressor, package_name = DecisionTree, ... )\n",
       " (name = DeterministicConstantRegressor, package_name = MLJModels, ... )\n",
       " (name = DummyRegressor, package_name = ScikitLearn, ... )\n",
       " (name = ElasticNetCVRegressor, package_name = ScikitLearn, ... )\n",
       " (name = ElasticNetRegressor, package_name = MLJLinearModels, ... )\n",
       " (name = ElasticNetRegressor, package_name = ScikitLearn, ... )\n",
       " (name = EpsilonSVR, package_name = LIBSVM, ... )\n",
       " (name = EvoTreeGaussian, package_name = EvoTrees, ... )\n",
       " ⋮\n",
       " (name = RandomForestRegressor, package_name = ScikitLearn, ... )\n",
       " (name = RidgeCVRegressor, package_name = ScikitLearn, ... )\n",
       " (name = RidgeRegressor, package_name = MLJLinearModels, ... )\n",
       " (name = RidgeRegressor, package_name = MultivariateStats, ... )\n",
       " (name = RidgeRegressor, package_name = ScikitLearn, ... )\n",
       " (name = RobustRegressor, package_name = MLJLinearModels, ... )\n",
       " (name = SGDRegressor, package_name = ScikitLearn, ... )\n",
       " (name = SVMLRegressor, package_name = ScikitLearn, ... )\n",
       " (name = SVMNuRegressor, package_name = ScikitLearn, ... )\n",
       " (name = SVMRegressor, package_name = ScikitLearn, ... )\n",
       " (name = TheilSenRegressor, package_name = ScikitLearn, ... )\n",
       " (name = XGBoostRegressor, package_name = XGBoost, ... )"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    " ms = models() do m Vector{Continuous}<: m.target_scitype end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining the Linear Model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#=\n",
    "X = copy(dfX)\n",
    "y = copy(dfYbinary)\n",
    "\n",
    "X=X[:,3:3]\n",
    "\n",
    "#ORIGINAL   LinearCountRegressor   LinearCountRegressor\n",
    "\n",
    "@pipeline LinearCountRegressorPipe(\n",
    "            std = Standardizer(),\n",
    "            hot = OneHotEncoder(drop_last = true),\n",
    "            reg = LinearCountRegressor()\n",
    ")\n",
    "\n",
    "coerce!(X, autotype(X, :string_to_multiclass))\n",
    "yc =  convert.(Int64,(y[:, 1]))\n",
    "yc = coerce(yc, Count)\n",
    "\n",
    "\n",
    "LogisticModel = machine(LinearCountRegressorPipe(), X, yc)\n",
    "fit!(LogisticModel)\n",
    "\n",
    "fp = fitted_params(LogisticModel).fitted_params\n",
    "ŷ = MLJ.predict(LogisticModel, X)\n",
    "#yhatResponse = [pdf(p, maximum(y)) for p in ŷ]\n",
    "#residuals = y .- yhatResponse\n",
    "\n",
    "r = report(LogisticModel)\n",
    "pprint(fp)\n",
    "pprint(r)\n",
    "println(\"\\nyhat\\n \", ŷ[1:5])\n",
    "=#\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading the Output of Fitting the Linear Model\n",
    "\n",
    "We can quickly define our models in MLJ.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining the Logistic Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#=\n",
    "X = copy(dfX)\n",
    "y = copy(dfYbinary)\n",
    "\n",
    "X=X[:,3:3]\n",
    "\n",
    "#ORIGINAL   LinearCountRegressor   LinearCountRegressor\n",
    "\n",
    "@pipeline LinearCountRegressorPipe(\n",
    "            std = Standardizer(),\n",
    "            hot = OneHotEncoder(drop_last = true),\n",
    "            reg = LinearCountRegressor()\n",
    ")\n",
    "\n",
    "coerce!(X, autotype(X, :string_to_multiclass))\n",
    "yc =  convert.(Int64,(y[:, 1]))\n",
    "yc = coerce(yc, Count)\n",
    "\n",
    "\n",
    "LogisticModel = machine(LinearCountRegressorPipe(), X, yc)\n",
    "fit!(LogisticModel)\n",
    "\n",
    "fp = fitted_params(LogisticModel).fitted_params\n",
    "ŷ = MLJ.predict(LogisticModel, X)\n",
    "#yhatResponse = [pdf(p, maximum(y)) for p in ŷ]\n",
    "#residuals = y .- yhatResponse\n",
    "\n",
    "r = report(LogisticModel)\n",
    "pprint(fp)\n",
    "pprint(r)\n",
    "println(\"\\nyhat\\n \", ŷ[1:5])\n",
    "=#\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_args = MLJBase.Source{:input}[\u001b[34mSource{:input} @ 1…19\u001b[39m]\n",
      "mach.model = \u001b[34mStandardizer @ 6…36\u001b[39m\n",
      "train_args = Node{NodalMachine{Standardizer}}[\u001b[34mNode{NodalMachine{Standardizer}} @ 7…09\u001b[39m]\n",
      "mach.model = \u001b[34mOneHotEncoder @ 1…81\u001b[39m\n",
      "train_args = AbstractNode[\u001b[34mNode{NodalMachine{OneHotEncoder}} @ 9…70\u001b[39m, \u001b[34mSource{:target} @ 2…11\u001b[39m]\n",
      "mach.model = \u001b[34mLinearBinaryClassifier{LogitLink} @ 5…27\u001b[39m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "┌ Info: Training \u001b[34mMachine{LinearBinaryClassifierPipe} @ 1…88\u001b[39m.\n",
      "└ @ MLJBase C:\\Users\\BCP\\.juliapro\\JuliaPro_v1.4.1-1\\packages\\MLJBase\\ESDzL\\src\\machines.jl:182\n",
      "┌ Info: Training \u001b[34mNodalMachine{Standardizer} @ 1…26\u001b[39m.\n",
      "└ @ MLJBase C:\\Users\\BCP\\.juliapro\\JuliaPro_v1.4.1-1\\packages\\MLJBase\\ESDzL\\src\\machines.jl:182\n",
      "┌ Info: Training \u001b[34mNodalMachine{OneHotEncoder} @ 1…84\u001b[39m.\n",
      "└ @ MLJBase C:\\Users\\BCP\\.juliapro\\JuliaPro_v1.4.1-1\\packages\\MLJBase\\ESDzL\\src\\machines.jl:182\n",
      "┌ Info: Spawning 1 sub-features to one-hot encode feature :gender.\n",
      "└ @ MLJModels C:\\Users\\BCP\\.juliapro\\JuliaPro_v1.4.1-1\\packages\\MLJModels\\gHake\\src\\builtins\\Transformers.jl:691\n",
      "┌ Info: Spawning 2 sub-features to one-hot encode feature :ethnicity.\n",
      "└ @ MLJModels C:\\Users\\BCP\\.juliapro\\JuliaPro_v1.4.1-1\\packages\\MLJModels\\gHake\\src\\builtins\\Transformers.jl:691\n",
      "┌ Info: Spawning 1 sub-features to one-hot encode feature :fcollege.\n",
      "└ @ MLJModels C:\\Users\\BCP\\.juliapro\\JuliaPro_v1.4.1-1\\packages\\MLJModels\\gHake\\src\\builtins\\Transformers.jl:691\n",
      "┌ Info: Spawning 1 sub-features to one-hot encode feature :mcollege.\n",
      "└ @ MLJModels C:\\Users\\BCP\\.juliapro\\JuliaPro_v1.4.1-1\\packages\\MLJModels\\gHake\\src\\builtins\\Transformers.jl:691\n",
      "┌ Info: Spawning 1 sub-features to one-hot encode feature :home.\n",
      "└ @ MLJModels C:\\Users\\BCP\\.juliapro\\JuliaPro_v1.4.1-1\\packages\\MLJModels\\gHake\\src\\builtins\\Transformers.jl:691\n",
      "┌ Info: Training \u001b[34mNodalMachine{LinearBinaryClassifier{LogitLink}} @ 8…65\u001b[39m.\n",
      "└ @ MLJBase C:\\Users\\BCP\\.juliapro\\JuliaPro_v1.4.1-1\\packages\\MLJBase\\ESDzL\\src\\machines.jl:182\n"
     ]
    },
    {
     "ename": "MethodError",
     "evalue": "MethodError: no method matching coef(::Tuple{GLM.GeneralizedLinearModel{GLM.GlmResp{Array{Float64,1},Distributions.Bernoulli{Float64},GLM.LogitLink},GLM.DensePredChol{Float64,LinearAlgebra.Cholesky{Float64,Array{Float64,2}}}},CategoricalValue{Int64,UInt32}})\nClosest candidates are:\n  coef(!Matched::Union{StatsModels.TableRegressionModel, StatsModels.TableStatisticalModel}, !Matched::Any...; kwargs...) at C:\\Users\\BCP\\.juliapro\\JuliaPro_v1.4.1-1\\packages\\StatsModels\\dvYSo\\src\\statsmodel.jl:28\n  coef(!Matched::GLM.LinPredModel) at C:\\Users\\BCP\\.juliapro\\JuliaPro_v1.4.1-1\\packages\\GLM\\6V3fS\\src\\linpred.jl:255\n  coef(!Matched::StatsBase.StatisticalModel) at C:\\Users\\BCP\\.juliapro\\JuliaPro_v1.4.1-1\\packages\\StatsBase\\548SN\\src\\statmodels.jl:10\n  ...",
     "output_type": "error",
     "traceback": [
      "MethodError: no method matching coef(::Tuple{GLM.GeneralizedLinearModel{GLM.GlmResp{Array{Float64,1},Distributions.Bernoulli{Float64},GLM.LogitLink},GLM.DensePredChol{Float64,LinearAlgebra.Cholesky{Float64,Array{Float64,2}}}},CategoricalValue{Int64,UInt32}})\nClosest candidates are:\n  coef(!Matched::Union{StatsModels.TableRegressionModel, StatsModels.TableStatisticalModel}, !Matched::Any...; kwargs...) at C:\\Users\\BCP\\.juliapro\\JuliaPro_v1.4.1-1\\packages\\StatsModels\\dvYSo\\src\\statsmodel.jl:28\n  coef(!Matched::GLM.LinPredModel) at C:\\Users\\BCP\\.juliapro\\JuliaPro_v1.4.1-1\\packages\\GLM\\6V3fS\\src\\linpred.jl:255\n  coef(!Matched::StatsBase.StatisticalModel) at C:\\Users\\BCP\\.juliapro\\JuliaPro_v1.4.1-1\\packages\\StatsBase\\548SN\\src\\statmodels.jl:10\n  ...",
      "",
      "Stacktrace:",
      " [1] fitted_params(::LinearBinaryClassifier{GLM.LogitLink}, ::Tuple{GLM.GeneralizedLinearModel{GLM.GlmResp{Array{Float64,1},Distributions.Bernoulli{Float64},GLM.LogitLink},GLM.DensePredChol{Float64,LinearAlgebra.Cholesky{Float64,Array{Float64,2}}}},CategoricalValue{Int64,UInt32}}) at C:\\Users\\BCP\\.juliapro\\JuliaPro_v1.4.1-1\\packages\\MLJModels\\gHake\\src\\GLM.jl:138",
      " [2] fitted_params(::NodalMachine{LinearBinaryClassifier{GLM.LogitLink}}) at C:\\Users\\BCP\\.juliapro\\JuliaPro_v1.4.1-1\\packages\\MLJBase\\ESDzL\\src\\operations.jl:45",
      " [3] iterate at .\\generator.jl:47 [inlined]",
      " [4] collect(::Base.Generator{Array{Any,1},typeof(fitted_params)}) at .\\array.jl:665",
      " [5] fitted_params(::Node{NodalMachine{LinearBinaryClassifier{GLM.LogitLink}}}) at C:\\Users\\BCP\\.juliapro\\JuliaPro_v1.4.1-1\\packages\\MLJBase\\ESDzL\\src\\composition\\composites.jl:51",
      " [6] fitted_params(::LinearBinaryClassifierPipe, ::Node{NodalMachine{LinearBinaryClassifier{GLM.LogitLink}}}) at C:\\Users\\BCP\\.juliapro\\JuliaPro_v1.4.1-1\\packages\\MLJBase\\ESDzL\\src\\composition\\composites.jl:55",
      " [7] fitted_params(::Machine{LinearBinaryClassifierPipe}) at C:\\Users\\BCP\\.juliapro\\JuliaPro_v1.4.1-1\\packages\\MLJBase\\ESDzL\\src\\operations.jl:45",
      " [8] top-level scope at In[12]:20"
     ]
    }
   ],
   "source": [
    "X = copy(dfX)\n",
    "y = copy(dfYbinary)\n",
    "\n",
    "X=X[:,1:6]\n",
    "\n",
    "# LinearBinaryClassifier  LinearBinaryClassifier  LinearBinaryClassifier  LinearBinaryClassifier \n",
    "\n",
    "@pipeline LinearBinaryClassifierPipe(\n",
    "            std = Standardizer(),\n",
    "            hot = OneHotEncoder(drop_last = true),\n",
    "            reg = LinearBinaryClassifier()\n",
    ")\n",
    "\n",
    "coerce!(X, autotype(X, :string_to_multiclass))\n",
    "yc = CategoricalArray(y[:, 1])\n",
    "yc = coerce(yc, OrderedFactor)\n",
    "\n",
    "LogisticModel = machine(LinearBinaryClassifierPipe(), X, yc)\n",
    "fit!(LogisticModel)\n",
    "fp = fitted_params(LogisticModel)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading the Output from the Prediction of the Logistic Model\n",
    "\n",
    "The output of the MLJ model basically contain the same information as the R version of the model.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========================================\n",
      "(machines = [\u001b[34mNodalMachine{LinearBinaryClassifier{LogitLink}} @ 8…65\u001b[39m,\n",
      "             \u001b[34mNodalMachine{OneHotEncoder} @ 1…84\u001b[39m,\n",
      "             \u001b[34mNodalMachine{Standardizer} @ 1…26\u001b[39m],\n",
      " reports =\n",
      "     [(deviance = 4503.761053428112,\n",
      "       dof_residual = 4732.0,\n",
      "       stderror = [0.07477390232879912,\n",
      "                   0.11990207348301012,\n",
      "                   0.10262164823735807,\n",
      "                   0.046157081532280576,\n",
      "                   0.09142354525679938,\n",
      "                   0.10583046238545038,\n",
      "                   0.10532017977348578,\n",
      "                   0.10867641079827785],\n",
      "       vcov =\n",
      "           [0.005591136469476789 0.00017773974164698634 0.0003484253454762139 0.0003870208943391252 -0.0002640475479755173 1.749586217660677e-5 -0.00018771823304335699 -0.003109293163132924; 0.00017773974164698634 0.014376507225525158 0.002861285440819615 0.001479363477803272 -0.0008537276458600929 0.0006027012612057836 -0.0011914050464634225 -0.0026616364001845086; 0.0003484253454762139 0.002861285440819615 0.010531202686952058 0.001046697217870831 -0.0006912343013098895 -0.0005566107193304949 -0.0007754727353543054 -0.001730687112499383; 0.0003870208943391252 0.001479363477803272 0.001046697217870831 0.002130476175577597 0.0002732701183831524 0.00016867410970200438 4.588325240493764e-5 -0.001793532504684497; -0.0002640475479755173 -0.0008537276458600929 -0.0006912343013098895 0.0002732701183831524 0.008358264627322046 -0.003784556510555551 -0.0003533720696037998 -0.0026440390092327343; 1.749586217660677e-5 0.0006027012612057836 -0.0005566107193304949 0.00016867410970200438 -0.003784556510555551 0.011200086768718228 -0.0002031648573594575 -0.006489527228166477; -0.00018771823304335699 -0.0011914050464634225 -0.0007754727353543054 4.588325240493764e-5 -0.0003533720696037998 -0.0002031648573594575 0.011092340267519362 -0.0008372377422424271; -0.003109293163132924 -0.0026616364001845086 -0.001730687112499383 -0.001793532504684497 -0.0026440390092327343 -0.006489527228166477 -0.0008372377422424271 0.011810562263996042]),\n",
      "      (features_to_be_encoded =\n",
      "           [:ethnicity, :fcollege, :home, :mcollege, :gender],\n",
      "       new_features = [:gender__female,\n",
      "                       :ethnicity__afam,\n",
      "                       :ethnicity__hispanic,\n",
      "                       :score,\n",
      "                       :fcollege__no,\n",
      "                       :mcollege__no,\n",
      "                       :home__no]),\n",
      "      (features_fit = [:score],)])\n",
      "========================================\n",
      "\n",
      " y \n",
      " [0, 0, 0, 0, 0]\n",
      "\n",
      " ŷ \n",
      " UnivariateFinite{Int64,UInt32,Float64}[UnivariateFinite(0=>0.917, 1=>0.0827), UnivariateFinite(0=>0.847, 1=>0.153), UnivariateFinite(0=>0.871, 1=>0.129), UnivariateFinite(0=>0.937, 1=>0.0625), UnivariateFinite(0=>0.949, 1=>0.051)]\n",
      "\n",
      " yhatResponse \n",
      " [0.9172629210714334, 0.8468033630657354, 0.8713728339664609, 0.9374533540266803, 0.948964692824914]\n",
      "\n",
      " Standard Error per Coefficient \n",
      "[0.07477390232879912, 0.11990207348301012, 0.10262164823735807, 0.046157081532280576, 0.09142354525679938, 0.10583046238545038, 0.10532017977348578, 0.10867641079827785]\n"
     ]
    }
   ],
   "source": [
    "ŷ = MLJ.predict(LogisticModel, X)\n",
    "yhatResponse = [pdf(ŷ[i], y[i,1]) for i in 1:nrow(y)]\n",
    "residuals = y .- yhatResponse\n",
    "r = report(LogisticModel)\n",
    "\n",
    "println(\"========================================\")\n",
    "pprint(r)\n",
    "println(\"\\n========================================\")\n",
    "#println(\"Coefficients:  \", fp[1].coef)\n",
    "println(\"\\n y \\n \", y[1:5,1])\n",
    "println(\"\\n ŷ \\n \", ŷ[1:5])\n",
    "println(\"\\n yhatResponse \\n \", yhatResponse[1:5])\n",
    "println(\"\\n Standard Error per Coefficient \\n\", r.reports[1].stderror)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OrderedFactor{2}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "elscitype(yc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.15377824025930306"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "yhatResponse = Float64[]\n",
    "i=3\n",
    "\n",
    "pdf(ŷ[i], 1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.4.1",
   "language": "julia",
   "name": "julia-1.4"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.4.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
