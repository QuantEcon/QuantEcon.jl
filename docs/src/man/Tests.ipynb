{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tests"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Testing code is very important. It is considered a good habit to write testing code and running this code in parallel. It helps developers define their code's intent and as well a have a more decoupled architecture.\n",
    "\n",
    "Another use of the testing code is as an introduction to new contributors. When someone has to work on the code base, running and reading the related testing code is often the best thing that they can do to start. They will discover the hot spots where most difficulties arise and the corner cases. If they have to add or fix some functionality, test-driven developement is generally a good practice: write and run simple, fast-running tests before writing the code itself and running tests again. Several iterations may be needed and leads to modularized, flexible and extensible code.\n",
    "\n",
    "If you are debugging code, the first step is to write a new test pinpointing the bug. While it is not always possible to do, those bug catching tests are among the most valuable pieces of code in any project.\n",
    "\n",
    "When something goes wrong or has to be fixed and if your code has a good set of tests, you or maintainers will rely largely on the tests to fix the problem or modify a given behavior. Therefore the testing code will be read as much as or even more than the running code. A unit test whose purpose is unclear is not very helpful in this case."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `QuantEcon.jl`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this part we're going to cover the conventions and procedures used by `QuantEcon.jl`.\n",
    "\n",
    "For a brief introduction to writing tests in `Julia` consult the [standard libary documentation](https://docs.julialang.org/en/stable/stdlib/test/)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### All tests should be located in `test/`.\n",
    "For example, suppose you want to test some functionality `foo`. You would create a file named `test_foo.jl`, write some tests in it and lastly add `include(test_foo.jl)` in `runtests.jl`. The same applies to other functionalities."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A testing unit should focus on one tiny bit of functionality and prove it correct.\n",
    "The intent of a unit test should be clear. A good unit test tells a story about some behavioral aspect of the code so it should be easy to understand which functionality is being tested and if the test fails: easy to detect how to address the problem."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use test macros.\n",
    "`@test` is used to test a condition and if `True` returns a `Pass`, if `False` it returns a `Fail` and throws an exception."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "using Base.Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "foo (generic function with 1 method)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "foo(x) = x + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[1m\u001b[32mTest Passed\u001b[39m\u001b[22m"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "@test foo(1) == 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m\u001b[91mTest Failed\n",
      "\u001b[39m\u001b[22m  Expression: foo(1) == 3\n",
      "   Evaluated: 2 == 3\n"
     ]
    },
    {
     "ename": "LoadError",
     "evalue": "\u001b[91mThere was an error during testing\u001b[39m",
     "output_type": "error",
     "traceback": [
      "\u001b[91mThere was an error during testing\u001b[39m",
      "",
      "Stacktrace:",
      " [1] \u001b[1mrecord\u001b[22m\u001b[22m\u001b[1m(\u001b[22m\u001b[22m::Base.Test.FallbackTestSet, ::Base.Test.Fail\u001b[1m)\u001b[22m\u001b[22m at \u001b[1m./test.jl:533\u001b[22m\u001b[22m",
      " [2] \u001b[1mdo_test\u001b[22m\u001b[22m\u001b[1m(\u001b[22m\u001b[22m::Base.Test.Returned, ::Expr\u001b[1m)\u001b[22m\u001b[22m at \u001b[1m./test.jl:352\u001b[22m\u001b[22m"
     ]
    }
   ],
   "source": [
    "@test foo(1) == 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can create a `@testset` which tests multiple conditions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[37mTesting foo: \u001b[39m\u001b[1m\u001b[91mTest Failed\n",
      "\u001b[39m\u001b[22m  Expression: foo(1) == 3\n",
      "   Evaluated: 2 == 3\n",
      "Stacktrace:\n",
      " [1] \u001b[1mmacro expansion\u001b[22m\u001b[22m at \u001b[1m./In[5]:2\u001b[22m\u001b[22m [inlined]\n",
      " [2] \u001b[1mmacro expansion\u001b[22m\u001b[22m at \u001b[1m./test.jl:860\u001b[22m\u001b[22m [inlined]\n",
      " [3] \u001b[1manonymous\u001b[22m\u001b[22m at \u001b[1m./<missing>:?\u001b[22m\u001b[22m\n",
      "\u001b[1m\u001b[37mTest Summary: | \u001b[39m\u001b[22m\u001b[1m\u001b[32mPass  \u001b[39m\u001b[22m\u001b[1m\u001b[91mFail  \u001b[39m\u001b[22m\u001b[1m\u001b[36mTotal\u001b[39m\u001b[22m\n",
      "Testing foo   | \u001b[32m   2  \u001b[39m\u001b[91m   1  \u001b[39m\u001b[36m    3\u001b[39m\n"
     ]
    },
    {
     "ename": "LoadError",
     "evalue": "\u001b[91mSome tests did not pass: 2 passed, 1 failed, 0 errored, 0 broken.\u001b[39m",
     "output_type": "error",
     "traceback": [
      "\u001b[91mSome tests did not pass: 2 passed, 1 failed, 0 errored, 0 broken.\u001b[39m",
      "",
      "Stacktrace:",
      " [1] \u001b[1mfinish\u001b[22m\u001b[22m\u001b[1m(\u001b[22m\u001b[22m::Base.Test.DefaultTestSet\u001b[1m)\u001b[22m\u001b[22m at \u001b[1m./test.jl:668\u001b[22m\u001b[22m",
      " [2] \u001b[1mmacro expansion\u001b[22m\u001b[22m at \u001b[1m./test.jl:867\u001b[22m\u001b[22m [inlined]",
      " [3] \u001b[1manonymous\u001b[22m\u001b[22m at \u001b[1m./<missing>:?\u001b[22m\u001b[22m"
     ]
    }
   ],
   "source": [
    "@testset \"Testing foo\" begin\n",
    "    @test foo(1) == 3\n",
    "    @test foo(1) == 2\n",
    "    @test foo(0.99999999) â‰ˆ 2\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If `foo` has multiple functionalities or use cases, you can create multiple test sets for each one of them. Test sets can also be nested."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If a test fails consistently it can be changed to use `@test_broken`. This will denote the test as `Broken` if the test continues to fail and alerts the user via an exception if the test succeeds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[1m\u001b[33mTest Broken\n",
       "\u001b[39m\u001b[22mExpression: foo(1) == 3\n"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "@test_broken foo(1) == 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you are in the middle of a development session and have to interrupt your work, it is a good idea to write a broken unit test about what you want to develop next. When coming back to work, you will have a pointer to where you were and get back on track faster."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.0.1",
   "language": "julia",
   "name": "julia-1.0"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.0.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
